柳超-逻辑回归：从入门到精通 知识点梳理
====
## Introduction

> 什么是逻辑回归？逻辑回归可以说是互联网领域应用最广的自动分类算法了：从单机运行的垃圾邮件自动石碑程序到需要成百上千台机器支持的互联网广告投放系统，算法主干都是LR。

> 逻辑回归看起来比较简单，其实简简单单了解算法是怎么一回事到工作当中遇到问题还是很容易碰壁的，比如说对于L1正则化在大规模LR模型中该怎么做算法优化，为啥你用BFGS跑的总是比别人慢等等。柳超大牛在《逻辑回归：从入门到精通》在内容上注重讲解对LR的理解以及背后的优化算法掌握。此外还将了最速梯度下降法，到牛顿方法，再到拟牛顿方法（DFP，BFGS，L-BFGS等算法）

## 初识逻辑回归

> 给定N个训练样本（x1, y1), (x2, y2), ..., (xN, yN), 其中xi是一个n维向量用来表示第i个样本在这n个特征上的取值，yi {-1, +1}表示了这个样本是正样本还是负样本。逻辑回归模型就是通过一个叫做sigmoid函数 ![PNG](../images/formula_sigmoid.png)将第i个样本的特征向量xi与该样本为正样本的概率联系了起来
> 
> 概率函数如下：<br>
<p(y_{i} = \pm 1| x_{i}, w) = \sigma (y_{i}{w}^Tx_{i}) = \frac{1}{1 + exp(-y_{i}{w}^Tx_{i})}>
> <center> ![PNG](../images/formula_lr_prob.png) </center>
> 这个公式是怎么来的呢？
> 由于sigmoid函数具有良好的性质，在这个函数就能够体现出来了:
> <center> ![PNG](../images/formula_lr_prob_2.png) </center>
> 这样一来就能够综合以上式子了。
 
### 评估数据的数据流变化过程
- **1**. 传入一组数据xi
- **2**. 参数矩阵w^T点乘xi再乘以 -1，得到一个分数值
- **3**. 利用sigmoid函数把该值压缩到0-1范围内，得到的数值就是该样本为正样本的概率
- **4**. 如果正样本概率大于0.5，该样本就是为正样本

### 目标函数
> 逻辑回归的目标就是要找到最好的w，使得正样本的分数比较大，同时负样本的分数比较小。想象一下，对于进来的数据计算正样本的概率值，最好的结果难道不是正样本算出来的概率都为1， 负样本算出来的概率都为0吗？严格来说，LR是通过最大似然估计原则来找到最好的w。<br><br>
> tips: 极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。




